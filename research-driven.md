# Research-Driven Investigation (RDI)

## The Knowledge Gap

For decades, research has been a fragmented, ad-hoc process. Researchers gather sources haphazardly, analyze data inconsistently, and produce reports that rarely trace back to rigorous methodology. Literature reviews are compiled after conclusions are formed. Citations are added as afterthoughts. Quality standards vary wildly from one investigation to the next. The "research" becomes whatever the researcher happened to find, filtered through whatever biases they brought to the investigation.

Research-Driven Investigation (RDI) inverts this structure. Methodology doesn't serve findings—findings serve methodology. Research questions aren't vague starting points; they're precise definitions that generate systematic investigation. Analysis frameworks aren't post-hoc justifications; they're predetermined approaches that produce transparent results. This isn't an incremental improvement to how we conduct research. It's a fundamental rethinking of what drives investigation.

The gap between research intent and execution has plagued knowledge work since its inception. We've tried to bridge it with better documentation, more detailed protocols, stricter review processes. These approaches fail because they accept the gap as inevitable. They try to narrow it but never eliminate it. RDI eliminates the gap by making research questions and methodology executable. When research questions generate structured investigation and methodology generates systematic analysis, there is no gap—only transformation.

This transformation is now possible because AI can understand complex research questions, design rigorous methodologies, and execute systematic investigations. But raw AI research without structure produces shallow, biased results. RDI provides that structure through research definitions, explicit methodologies, and transparent analysis that are precise, complete, and unambiguous enough to generate credible findings. The research question becomes the primary artifact. The report becomes its expression through a particular methodology and evidence base.

In this new world, conducting research means executing methodology. Knowledge synthesis is expressed through structured investigation—the lingua franca of research moves to systematic, reproducible processes, and reports are the last-mile deliverable. Updating research with new evidence or exploring alternative interpretations means revisiting the research definition and methodology. This process is iterative, transparent, and continuously improvable.

## The RDI Workflow in Practice

The workflow begins with a question—often broad and exploratory. Through iterative dialogue with AI, this question becomes a comprehensive research definition with clear scope, objectives, and success criteria. The AI helps identify edge cases, clarify ambiguities, and define precise research boundaries. What might take days of proposal writing in traditional research happens in hours of focused definition work.

When a researcher updates research questions, methodology automatically flags affected data sources and analysis approaches. When new evidence emerges, the research definition updates to reflect expanded scope and refined questions. Throughout this process, research principles establish quality standards—your organization's source credibility requirements, ethical guidelines, and citation standards seamlessly integrate into every investigation.

From the research definition, AI generates methodologies that map questions to data sources, collection methods, and analysis frameworks. Every methodological choice has documented rationale. Every source selection traces back to specific research questions. Methodology validation continuously improves quality. AI analyzes methodologies for rigor, feasibility, and ethical considerations—not as a one-time gate, but as ongoing refinement.

Investigation begins as soon as methodologies are stable enough, but they do not have to be "complete." Early searches might be exploratory—testing whether the methodology makes sense in practice. Research questions become literature searches. Analysis frameworks become synthesis structures. Quality criteria become validation checklists. This merges investigation and validation through methodology—quality checks aren't performed after research, they're part of the methodology that generates both findings and verification.

The feedback loop extends beyond initial investigation. New sources and contradictory evidence don't just trigger addendums—they update methodologies for the next iteration. Knowledge gaps become refined research questions. Bias discoveries become constraints that affect all future investigations. This iterative dance between question, methodology, and evidence is where true understanding emerges.

## Why RDI Matters Now

Three trends make RDI not just possible but necessary:

First, AI capabilities have reached a threshold where natural language research questions can reliably generate systematic investigations. This isn't about replacing researchers—it's about amplifying their effectiveness by automating the mechanical aspects of literature review, source gathering, and citation management while maintaining rigorous quality standards.

Second, information complexity continues to grow exponentially. Modern research must synthesize dozens of sources across disciplines, evaluate conflicting claims, and navigate methodological debates. Keeping all these pieces aligned with original research intent through manual processes becomes increasingly difficult. RDI provides systematic alignment through methodology-driven investigation.

Third, the pace of knowledge evolution accelerates. Research becomes outdated more rapidly today than ever before. New evidence emerges constantly. Pivoting research focus is no longer exceptional—it's expected. Modern research demands rapid iteration based on emerging findings, methodological insights, and evolving questions.

Traditional research treats scope changes as disruptions. Each pivot requires manually updating literature reviews, revising frameworks, and reconciling citations. The result is either slow, careful updates that limit agility, or fast, reckless changes that accumulate methodological debt and introduce bias.

RDI transforms scope evolution from obstacle into normal workflow. When research questions drive investigation, pivots become systematic regenerations rather than manual rewrites. Change a research question in the definition, and affected methodology updates automatically. Modify scope boundaries, and corresponding data sources regenerate. This isn't just about initial research—it's about maintaining research velocity through inevitable scope evolution.

## Core Principles

**Research Questions as the Lingua Franca**: The research definition becomes the primary artifact. Investigation becomes its expression through a particular methodology and evidence base. Conducting research means executing methodology.

**Executable Methodologies**: Methodologies must be precise, complete, and unambiguous enough to generate systematic investigations. This eliminates the gap between intent and execution.

**Continuous Quality Assurance**: Validation happens continuously, not as a one-time gate. AI analyzes methodologies and findings for rigor, bias, and gaps as an ongoing process.

**Evidence-Driven Context**: Source evaluation and credibility assessment happen throughout the investigation process, not just during literature review.

**Transparent Analysis**: Analysis frameworks and interpretation approaches are documented explicitly before findings are generated, enabling reproducibility and bias detection.

**Methodology Branching**: Generate multiple investigation approaches from the same research question to explore different analytical frameworks, theoretical lenses, or methodological traditions.

## Implementation Approaches

Today, practicing RDI requires assembling existing tools and maintaining discipline throughout the process. The methodology can be practiced with:

- AI assistants for iterative research definition development
- Systematic literature search for gathering academic and professional sources
- Citation management tools for maintaining references (BibTeX format)
- Methodology validation through AI analysis
- Quality checking through explicit credibility criteria

The key is treating research questions as the source of truth, with findings as the generated output that serves the methodology rather than the other way around.

## Streamlining RDI with Commands

The RDI methodology is significantly enhanced through powerful commands that automate the research definition → methodology → execution workflow:

### The `/research.principles` Command

This command establishes the quality standards and ethical guidelines that govern all research:

1. **Source Quality Standards**: Defines criteria for credibility, peer-review requirements, recency expectations
2. **Methodology Rigor**: Establishes requirements for research design, assumption documentation, limitation acknowledgment
3. **Ethics & Bias**: Sets ethical guidelines, conflict of interest disclosure, bias mitigation strategies
4. **Citation Standards**: Specifies attribution requirements, citation format (BibTeX), source accessibility needs
5. **Analysis Quality**: Requires transparency in frameworks, data interpretation approaches, consideration of alternatives
6. **Report Completeness**: Defines requirements for executive summaries, methodology documentation, findings support

### The `/research.define` Command

This command transforms a research topic into a complete, structured research definition with automatic repository management:

1. **Automatic Research Numbering**: Scans existing research directories to determine the next research number (e.g., 001, 002, 003)
2. **Branch Creation**: Generates a semantic branch name from your research topic and creates it automatically
3. **Template-Based Generation**: Copies and customizes the research definition template with your requirements
4. **Directory Structure**: Creates the proper `research/[branch-name]/` structure for all related documents

### The `/research.refine` Command (Optional)

This optional command helps clarify ambiguous aspects of the research definition:

1. **Structured Clarification**: Asks targeted questions about scope, objectives, and success criteria
2. **Coverage-Based**: Ensures all critical aspects are well-defined
3. **Recorded Answers**: Updates definition.md with clarifications section
4. **Quality Improvement**: Reduces risk of methodological issues downstream

### The `/research.methodology` Command

Once a research definition exists, this command creates a comprehensive investigation plan:

1. **Definition Analysis**: Reads and understands research questions, objectives, and scope
2. **Principles Compliance**: Ensures alignment with research principles and quality standards
3. **Methodological Design**: Determines research design, data sources, collection methods, and analysis frameworks
4. **Supporting Documentation**: Generates literature-review.md, data-sources.md, and references.bib
5. **Timeline & Phases**: Produces realistic timeline with milestones and checkpoints

### The `/research.validate` Command (Optional)

After methodology is created, this command validates feasibility and rigor:

1. **Feasibility Check**: Assesses whether proposed methodology is realistic given constraints
2. **Rigor Review**: Validates that methodology meets quality standards from research principles
3. **Ethics Review**: Checks for ethical considerations and bias mitigation strategies
4. **Resource Assessment**: Evaluates whether required tools, sources, and time are available
5. **Output**: Produces validation-report.md with recommendations

### The `/research.tasks` Command

After methodology is validated, this command breaks down investigation into actionable tasks:

1. **Methodology Analysis**: Reads methodology, literature review structure, and data sources
2. **Task Derivation**: Converts research phases into specific, executable tasks
3. **Phase Organization**: Groups tasks by investigation phase with clear dependencies
4. **Parallelization**: Marks independent tasks for parallel execution
5. **Output**: Writes tasks.md in the research directory, ready for execution

### The `/research.execute` Command

With tasks defined, this command conducts the research and compiles the report:

1. **Task Execution**: Follows task breakdown systematically
2. **Literature Collection**: Gathers sources according to methodology
3. **Citation Management**: Maintains references.bib with proper BibTeX entries
4. **Analysis Application**: Applies analytical frameworks as defined in methodology
5. **Report Generation**: Produces report.md and findings.md with full documentation

### Example: Investigating AI Market Trends

Here's how these commands transform the traditional research workflow:

**Traditional Approach:**

```text
1. Start with vague topic and Google search (2-3 hours)
2. Collect random sources without clear criteria (4-5 hours)
3. Read sources and take unstructured notes (8-10 hours)
4. Try to synthesize findings without clear framework (4-6 hours)
5. Write report and backfill citations (3-4 hours)
Total: ~25 hours of unfocused investigation
```

**RDI with Commands Approach:**

```bash
# Step 1: Establish research principles (10 minutes)
/research.principles Focus on peer-reviewed sources, recent publications (last 3 years), transparent methodology, ethical AI considerations, and BibTeX citations

# Step 2: Define the research (10 minutes)
/research.define Analyze AI market trends in healthcare, focusing on adoption barriers, ROI evidence, and regulatory challenges

# This automatically:
# - Creates branch "001-ai-healthcare-trends"
# - Generates research/001-ai-healthcare-trends/definition.md
# - Defines research questions, objectives, scope

# Step 3: Design methodology (10 minutes)
/research.methodology

# This automatically creates:
# - research/001-ai-healthcare-trends/methodology.md
# - research/001-ai-healthcare-trends/literature-review.md (structure)
# - research/001-ai-healthcare-trends/data-sources.md
# - research/001-ai-healthcare-trends/references.bib (initialized)

# Step 4: Break down into tasks (5 minutes)
/research.tasks

# Step 5: Execute research (varies by scope)
/research.execute
```

In 35 minutes of definition and planning work, you have:

- A complete research definition with clear questions and scope
- A rigorous methodology with explicit data sources and analysis frameworks
- Literature review structure ready for systematic population
- Citation management system initialized (BibTeX)
- Task breakdown for systematic execution
- All documents properly versioned in a research branch

### The Power of Structured Investigation

These commands don't just save time—they enforce consistency and completeness:

1. **No Forgotten Steps**: Templates ensure every aspect is considered, from ethics review to limitation acknowledgment
2. **Traceable Decisions**: Every methodological choice links back to specific research questions
3. **Living Documentation**: Research definitions stay in sync with findings because they generate them
4. **Rapid Iteration**: Change research questions and regenerate methodology in minutes, not days

The commands embody RDI principles by treating research questions as executable artifacts rather than static documents. They transform the research process from a necessary exploration into the driving force of systematic investigation.

### Template-Driven Quality: How Structure Constrains AI for Better Research

The true power of these commands lies not just in automation, but in how the templates guide AI behavior toward higher-quality research. The templates act as sophisticated prompts that constrain the AI's output in productive ways:

#### 1. **Preventing Premature Conclusions**

The research definition template explicitly instructs:

```text
- ✅ Focus on WHAT you want to know and WHY
- ❌ Avoid jumping to conclusions or stating expected findings
```

This constraint forces the AI to maintain proper objectivity. When an AI might naturally jump to "AI adoption is increasing in healthcare," the template keeps it focused on "what factors influence AI adoption rates in healthcare settings." This separation ensures research remains open to contradictory evidence.

#### 2. **Forcing Explicit Uncertainty Markers**

Templates mandate the use of `[NEEDS CLARIFICATION]` markers:

```text
When creating research definition from a user topic:
1. **Mark all ambiguities**: Use [NEEDS CLARIFICATION: specific question]
2. **Don't assume**: If the topic doesn't specify scope, mark it
3. **Limit**: Maximum 3 clarification markers total
```

This prevents the common AI behavior of making plausible but potentially incorrect assumptions. Instead of guessing that "market analysis" means "global market," the AI must mark it as `[NEEDS CLARIFICATION: geographic scope - global, regional, or specific countries?]`.

#### 3. **Structured Thinking Through Research Types**

The templates include clear categorization that acts as decision framework:

```text
Research Types:
- Academic Research (theory, literature review, scholarly contribution)
- Market/Business Research (competitive intelligence, trends, business insights)
- Technical Research (technology evaluation, feasibility, comparisons)
- General Investigation (topic exploration, knowledge synthesis)
```

These categories force the AI to think systematically about methodology selection. Different research types require different source evaluation criteria, analytical frameworks, and output standards.

#### 4. **Methodology Rigor Through Phases**

The methodology template enforces systematic planning:

```markdown
### Required Methodology Sections
A. Research Design (qualitative/quantitative/mixed, justification)
B. Data Sources (primary/secondary, quality criteria)
C. Data Collection Methods (search strategy, inclusion/exclusion)
D. Analysis Approach (frameworks, techniques, synthesis)
E. Tools & Platforms (citation management, analysis software)
F. Ethical Considerations (privacy, bias mitigation, conflicts)
G. Timeline & Phases (milestones, checkpoints)
H. Limitations (acknowledged constraints)
```

These required sections prevent methodological shortcuts. The AI cannot skip ethics review or limitation acknowledgment—it must address each systematically.

#### 5. **Citation Quality Through BibTeX**

The templates enforce proper citation management:

```text
All sources MUST be cited in BibTeX format in references.bib
Citation requirements:
- Author(s), Title, Year, Publication
- DOI or URL for accessibility
- Access date for web sources
- Peer review status noted
```

This constraint ensures the AI maintains proper attribution throughout research, not as an afterthought. Every claim traces back to a specific, verifiable source.

#### 6. **Source Credibility Evaluation**

The methodology template requires explicit source quality criteria:

```text
Source Quality Framework:
- Primary vs. secondary source distinction
- Peer-review requirements
- Recency criteria (e.g., last 3 years for technical research)
- Bias identification approach
- Conflicting evidence handling protocol
```

This forces the AI to apply consistent quality standards rather than cherry-picking convenient sources. The methodology documents the credibility threshold before sources are gathered.

#### 7. **Preventing Scope Creep**

Templates enforce clear boundaries:

```text
Scope Definition:
- What's included (explicit boundaries)
- What's excluded (explicit out-of-scope items)
- Time boundaries (if applicable)
- Geographic/domain boundaries (if applicable)
```

This stops the AI from expanding research scope without explicit justification. Every addition must be weighed against defined boundaries.

### The Compound Effect

These constraints work together to produce research that is:

- **Rigorous**: Methodology documented before execution
- **Unbiased**: Source evaluation criteria explicit and transparent
- **Traceable**: Every finding links back to specific sources
- **Reproducible**: Clear methodology enables validation
- **Complete**: Templates ensure nothing is forgotten

The templates transform the AI from a knowledge aggregator into a disciplined research partner, channeling its capabilities toward producing consistently high-quality, systematic investigations that meet academic and professional research standards.

## The Research Principles Foundation: Enforcing Quality Standards

At the heart of RDI lies research principles—a set of quality standards that govern how research questions become credible findings. The research principles (`/principles/research-principles.md`) act as the quality DNA of the investigation, ensuring that every piece of research maintains consistency, rigor, and ethical standards.

### The Six Pillars of Research Quality

The research principles define six pillars that shape every aspect of the investigation:

#### Pillar I: Source Quality Standards

Every research project must define clear criteria for source credibility:

```text
Primary vs. Secondary Sources:
- When to require primary sources (original research, data)
- When secondary sources are acceptable (reviews, meta-analyses)

Peer Review Requirements:
- Academic research: Peer-reviewed journals strongly preferred
- Technical research: Mix of peer-reviewed and industry sources
- Market research: Credible industry reports with transparent methodology

Currency Standards:
- Technical topics: Last 3 years preferred, last 5 years maximum
- Established theory: Seminal works accepted regardless of age
- Market trends: Last 2 years strongly preferred
```

This principle ensures AI cannot use low-quality or outdated sources without explicit justification.

#### Pillar II: Methodology Rigor

Research methodology must be explicit and defensible:

```text
Research Design:
- Clear research design (qualitative/quantitative/mixed)
- Justification for chosen approach
- Alternative approaches considered

Assumptions Documentation:
- All assumptions stated explicitly
- Impact of assumptions on findings acknowledged
- Alternative interpretations considered

Limitations Acknowledgment:
- Methodological limitations stated clearly
- Resource constraints documented
- Scope boundaries justified
```

This prevents the AI from using implicit assumptions or ignoring methodological weaknesses.

#### Pillar III: Ethics & Bias

Ethical considerations and bias mitigation are mandatory:

```text
Ethical Guidelines:
- Privacy protection in data collection
- Informed consent (if human subjects)
- Data handling and storage protocols

Conflict of Interest:
- Financial relationships disclosed
- Professional biases acknowledged
- Funding sources transparent

Bias Mitigation:
- Confirmation bias prevention strategies
- Source diversity requirements
- Contradictory evidence acknowledgment
- Multiple perspective representation
```

This ensures the AI conducts research with intellectual honesty and ethical rigor.

#### Pillar IV: Citation Standards

Proper attribution is non-negotiable:

```text
Citation Requirements:
- BibTeX format for all references
- Complete citation information (author, title, year, publication)
- DOI or URL for accessibility
- Access date for web sources

Attribution Standards:
- Direct quotes clearly marked and cited
- Paraphrased content attributed to source
- Ideas and frameworks credited to originators
- No assertion without source or explicit marking as inference
```

This prevents the AI from making unsupported claims or failing to credit sources.

#### Pillar V: Analysis Quality

Analysis must be transparent and rigorous:

```text
Framework Transparency:
- Analytical frameworks stated explicitly before analysis
- Theoretical lenses identified and justified
- Alternative frameworks considered

Data Interpretation:
- Interpretation process documented
- Evidence for interpretations provided
- Confidence levels stated (high/medium/low)

Alternative Explanations:
- Contradictory evidence acknowledged
- Alternative interpretations considered
- Limitations of conclusions stated clearly
```

This ensures the AI doesn't cherry-pick evidence or ignore complexity.

#### Pillar VI: Report Completeness

Research reports must meet comprehensive standards:

```text
Executive Summary:
- Research questions restated
- Key findings highlighted
- Conclusions summarized
- Recommendations provided (if applicable)

Methodology Documentation:
- Research design described
- Data sources listed with quality assessment
- Collection methods explained
- Analysis approach detailed

Findings Support:
- All findings linked to specific evidence
- Source citations for all claims
- Confidence levels stated
- Contradictions acknowledged

Conclusions Justification:
- Conclusions supported by findings
- Limitations of conclusions stated
- Alternative interpretations noted
- Future research directions suggested
```

This ensures research reports are complete, credible, and useful.

### Principles Enforcement Through Templates

The research methodology template operationalizes these principles through concrete requirements:

```markdown
### Methodology Quality Checklist
#### Source Quality (Pillar I)
- [ ] Source credibility criteria defined
- [ ] Peer-review requirements specified
- [ ] Currency standards stated

#### Methodology Rigor (Pillar II)
- [ ] Research design explicit and justified
- [ ] Assumptions documented
- [ ] Limitations acknowledged

#### Ethics & Bias (Pillar III)
- [ ] Ethical considerations addressed
- [ ] Bias mitigation strategies defined
- [ ] Conflicts of interest disclosed

#### Citation Standards (Pillar IV)
- [ ] BibTeX format specified
- [ ] Attribution requirements clear
- [ ] references.bib initialized

#### Analysis Quality (Pillar V)
- [ ] Analytical frameworks stated
- [ ] Interpretation process defined
- [ ] Alternative explanations required

#### Report Completeness (Pillar VI)
- [ ] Report structure defined
- [ ] Findings documentation standards set
- [ ] Conclusions requirements stated
```

These checklists act as quality gates for research methodology. The AI cannot proceed without either meeting the standards or documenting justified exceptions.

### The Power of Explicit Standards

The research principles' power lies in their explicitness. While research quality can be subjective, the principles make expectations concrete. This provides:

1. **Consistency Across Investigations**: Research conducted today follows the same standards as research conducted next month
2. **Consistency Across Topics**: Different research topics maintain comparable quality standards
3. **Quality Assurance**: Principles provide objective criteria for evaluating research quality
4. **Transparency**: Anyone can understand what quality standards were applied

### Principles Evolution

While principles establish standards, they can evolve based on experience:

```text
Principles Amendment Process:
- Explicit documentation of rationale for change
- Version increment (semantic versioning)
- Impact assessment on existing research
- Consistency propagation to templates
```

This allows the methodology to learn and improve while maintaining stability. The principles document their own evolution with version history, demonstrating how quality standards can be refined based on practical experience.

### Beyond Rules: A Research Philosophy

The research principles aren't just a rulebook—they're a philosophy that shapes how AI thinks about investigation:

- **Rigor Over Convenience**: Use high-quality sources even if harder to access
- **Transparency Over Impression**: Document limitations instead of hiding them
- **Evidence Over Assumption**: Cite sources for claims instead of stating opinions
- **Ethics Over Expedience**: Consider ethical implications even when not legally required

By embedding these principles into the research definition and methodology process, RDI ensures that generated research isn't just comprehensive—it's credible, ethical, and intellectually honest. The principles transform AI from a content aggregator into a research partner that respects and reinforces quality standards.

## Research Types Deep Dive

RDI supports four primary research types, each with distinct characteristics, methodologies, and outputs:

### Academic Research

**When to Use**:
- Contributing to scholarly knowledge
- Building on theoretical frameworks
- Publishing in academic journals
- Dissertation or thesis work
- Literature-based investigations

**Characteristics**:
- Extensive literature review (20+ sources typical)
- Theoretical framework explicit
- Peer-reviewed sources heavily preferred
- Rigorous methodology with clear research design
- Limitation acknowledgment prominent
- Citations abundant (BibTeX format)

**Typical Outputs**:
- Literature review document (10-20 pages)
- Theoretical framework section
- Methodology document (detailed)
- Findings with statistical or thematic analysis
- Final research report (academic paper structure)
- Comprehensive references.bib

**Quality Standards**:
- Peer-reviewed sources: 80%+ of references
- Currency: Seminal works + recent research (last 5 years)
- Methodology: Explicit research design with justification
- Analysis: Transparent framework with alternative explanations

### Market Research

**When to Use**:
- Analyzing market opportunities
- Competitive intelligence gathering
- Business strategy development
- Customer insights synthesis
- Trend analysis for decision-making

**Characteristics**:
- Mix of quantitative and qualitative data
- Industry reports and whitepapers prominent
- Focus on actionable insights
- SWOT analysis, Porter's 5 Forces, or similar frameworks
- ROI and business impact emphasis
- Practical recommendations

**Typical Outputs**:
- Market analysis report
- Competitive landscape overview
- Trend analysis with forecasts
- Strategic recommendations
- Key metrics and benchmarks
- Executive summary for stakeholders

**Quality Standards**:
- Source credibility: Reputable firms (Gartner, Forrester, McKinsey, etc.)
- Currency: Last 2 years strongly preferred for trends
- Methodology: Transparent data collection and analysis approach
- Bias mitigation: Multiple perspectives, conflicting reports acknowledged

### Technical Research

**When to Use**:
- Evaluating technology options
- Architectural decision-making
- Proof-of-concept planning
- Technology feasibility assessment
- Tool/framework comparisons

**Characteristics**:
- Mix of documentation, benchmarks, and hands-on evaluation
- Technical specifications and API documentation
- Performance comparisons and trade-off analysis
- Proof-of-concept code or prototypes
- Practical feasibility focus
- Clear technical recommendations

**Typical Outputs**:
- Technology evaluation report
- Comparison matrices (features, performance, cost)
- Architecture recommendations
- Proof-of-concept documentation
- Implementation considerations
- Risk assessment

**Quality Standards**:
- Source credibility: Official documentation + credible technical sources
- Currency: Last 3 years (technical topics evolve rapidly)
- Methodology: Hands-on evaluation + literature review
- Transparency: Trade-offs acknowledged, no "silver bullet" claims

### General Investigation

**When to Use**:
- Broad topic exploration
- Knowledge synthesis across domains
- Due diligence research
- Fact-checking and verification
- Background research for projects

**Characteristics**:
- Exploratory and comprehensive
- Diverse source types (academic, industry, news, government)
- Knowledge mapping and synthesis
- Multiple perspectives represented
- Breadth over depth
- Accessible language

**Typical Outputs**:
- Comprehensive investigation report
- Knowledge map or concept diagram
- Key findings summary
- Source credibility assessment
- Future research directions
- Annotated bibliography

**Quality Standards**:
- Source diversity: Multiple types and perspectives
- Currency: Depends on topic (recent for current events, historical for established topics)
- Methodology: Systematic exploration with clear scope
- Balance: Contradictory views represented fairly

### Choosing the Right Research Type

**Decision Framework**:

1. **Output Destination**:
   - Academic journal/thesis → Academic Research
   - Business decision-making → Market Research
   - Technology selection → Technical Research
   - General understanding → General Investigation

2. **Audience**:
   - Scholars/academics → Academic Research
   - Business stakeholders → Market Research
   - Technical teams → Technical Research
   - General readers → General Investigation

3. **Source Availability**:
   - Abundant peer-reviewed literature → Academic Research
   - Rich industry reports → Market Research
   - Documentation + community resources → Technical Research
   - Mixed sources → General Investigation

4. **Time & Resources**:
   - Extensive time, deep rigor → Academic Research
   - Moderate time, actionable focus → Market Research
   - Moderate time, hands-on evaluation → Technical Research
   - Limited time, broad overview → General Investigation

The research type selection happens in `/research.define` and influences:
- Source quality criteria (Pillar I)
- Methodology rigor requirements (Pillar II)
- Citation standards (Pillar IV)
- Analysis frameworks (Pillar V)
- Report structure (Pillar VI)

## Quality Assurance in RDI

Quality assurance is not a final step—it's woven throughout the RDI workflow:

### Source Credibility Checks

**When**: During literature collection (Phase 2 of `/research.tasks`)

**How**:
- Apply source quality criteria from methodology
- Assess peer-review status
- Evaluate author credentials and affiliations
- Check publication venue reputation
- Verify recency against currency standards
- Identify potential conflicts of interest

**Tools**:
- BibTeX metadata in references.bib
- data-sources.md with credibility notes
- Methodology's source quality criteria as rubric

### Methodology Validation

**When**: After `/research.methodology` (optional `/research.validate`)

**How**:
- Feasibility check: Are data sources accessible?
- Rigor check: Does methodology meet research principles?
- Ethics check: Are ethical considerations addressed?
- Resource check: Are timeline and tools realistic?

**Tools**:
- validation-report.md (if using `/research.validate`)
- Methodology quality checklist
- Research principles compliance review

### Citation Verification

**When**: Continuously during research execution, final check before report completion

**How**:
- All claims traced to specific sources in references.bib
- BibTeX entries complete and correctly formatted
- Direct quotes clearly marked with page numbers
- Paraphrased content attributed appropriately
- No unsupported assertions

**Tools**:
- references.bib (BibTeX format)
- Citation management workflow in methodology
- Report quality checklist

### Peer Review (Optional)

**When**: After draft report completion, before finalization

**How**:
- Another researcher reviews methodology and findings
- Checks for bias, gaps, and unsupported conclusions
- Validates source credibility and citation quality
- Provides feedback on clarity and completeness

**Tools**:
- Peer review template (if organizational standard exists)
- Research principles as review rubric
- Methodology as baseline for expectations

### Research Principles Compliance

**When**: Throughout workflow, final validation before report completion

**How**:
- Check source quality standards (Pillar I)
- Validate methodology rigor (Pillar II)
- Review ethics and bias mitigation (Pillar III)
- Verify citation standards (Pillar IV)
- Assess analysis quality (Pillar V)
- Confirm report completeness (Pillar VI)

**Tools**:
- Research principles document
- Quality checklists in templates
- Methodology compliance sections

### Continuous Quality Improvement

**Feedback Loop**:
1. Conduct research using RDI methodology
2. Document quality issues encountered
3. Update research principles to address issues
4. Refine templates to enforce improvements
5. Apply updated standards to next research project

**Version Control**:
- Research principles versioned semantically
- Template updates traced to principle changes
- Methodology improvements documented in principles history

## Best Practices for RDI

### Start with Clear Research Questions

**Do**:
- "What factors influence AI adoption rates in healthcare settings?"
- "How do remote work policies impact employee productivity across industries?"
- "What are the trade-offs between React and Vue.js for enterprise applications?"

**Don't**:
- "AI in healthcare" (too vague)
- "Remote work" (no clear question)
- "Which framework is best?" (presumes single answer)

**Why**: Clear research questions enable focused methodology, appropriate scope, and meaningful findings.

### Document Assumptions Early

**Practice**:
- State assumptions explicitly in methodology
- Acknowledge impact of assumptions on findings
- Consider alternative assumptions and their implications

**Example**:
```markdown
## Assumptions
1. Market size estimates from Gartner (2024) are reliable
   - Impact: Revenue projections depend on these estimates
   - Alternative: Cross-check with Forrester and IDC reports

2. Survey respondents represent broader population
   - Impact: Generalizability of findings
   - Alternative: Acknowledge sampling limitations in conclusions
```

**Why**: Explicit assumptions enable readers to assess research validity and applicability.

### Cite as You Go (BibTeX)

**Practice**:
- Add sources to references.bib immediately when found
- Include full metadata (author, title, year, DOI/URL)
- Link findings to specific BibTeX keys in notes
- Use citation management from the start, not as cleanup task

**Tools**:
- Zotero, Mendeley, or similar with BibTeX export
- BibTeX keys consistent (e.g., AuthorYearShortTitle)
- references.bib version controlled with research documents

**Why**: Real-time citation prevents lost sources, missing attributions, and cleanup headaches.

### Acknowledge Limitations

**Practice**:
- Document methodological limitations honestly
- State scope boundaries clearly
- Acknowledge contradictory evidence
- Note resource constraints that affected research

**Example**:
```markdown
## Limitations
1. Literature review limited to English-language sources
   - May miss relevant non-English research in European markets

2. Time constraints prevented hands-on evaluation
   - Recommendations based on documentation and secondary sources

3. Conflicting evidence on performance benchmarks
   - Vendor-supplied benchmarks may not reflect real-world scenarios
```

**Why**: Limitation acknowledgment demonstrates intellectual honesty and helps readers assess finding reliability.

### Consider Alternative Explanations

**Practice**:
- When findings support a hypothesis, actively seek contradictory evidence
- Present alternative interpretations of data
- Acknowledge where evidence is ambiguous
- State confidence levels (high/medium/low)

**Example**:
```markdown
## Finding: Increased AI adoption in radiology

Interpretation A (Primary): Improved accuracy and efficiency drive adoption
- Evidence: 3 studies show diagnostic accuracy improvements (Smith 2023, Jones 2024, Lee 2024)

Interpretation B (Alternative): Reimbursement incentives drive adoption
- Evidence: Policy changes in 2023 increased reimbursement for AI-assisted diagnostics (Medicare 2023)

Assessment: Both factors likely contribute; disentangling causation requires longitudinal study (not available)
```

**Why**: Considering alternatives prevents confirmation bias and strengthens research credibility.

### Keep Methodology Transparent

**Practice**:
- Document data collection process explicitly
- Describe analysis frameworks before applying them
- Show how conclusions derive from evidence
- Enable reproducibility

**Transparency Checklist**:
- [ ] Research questions stated clearly
- [ ] Data sources listed with accessibility information
- [ ] Collection methods described step-by-step
- [ ] Inclusion/exclusion criteria explicit
- [ ] Analysis frameworks defined before findings
- [ ] Interpretation process documented

**Why**: Transparency enables validation, reproducibility, and trustworthiness.

### Update Literature Review Continuously

**Practice**:
- Treat literature review as living document, not one-time task
- Add new sources as discovered during research
- Refine categorization as themes emerge
- Maintain literature-review.md alongside research execution

**Structure**:
```markdown
## Literature Review

### Theme 1: AI Diagnostic Accuracy
- Smith et al. (2023): Meta-analysis of 50 studies...
- Jones et al. (2024): Prospective study in 10 hospitals...
[Updated: 2024-11-12]

### Theme 2: Adoption Barriers
- Lee et al. (2024): Survey of 200 radiologists...
[Updated: 2024-11-10]
```

**Why**: Continuous updates prevent "literature lockdown" where new evidence is ignored after methodology design.

### Use Version Control for Research

**Practice**:
- Commit research definition, methodology, and findings to git
- Use feature branches for research projects (001-topic-name)
- Document major revisions in commit messages
- Tag major milestones (definition complete, methodology approved, report finalized)

**Benefits**:
- Track evolution of research questions and scope
- Revert to prior methodology if pivot doesn't work
- Collaborate with multiple researchers
- Demonstrate research process transparency

## Common Pitfalls in RDI

### Vague Research Questions

**Problem**: "Research AI ethics"

**Why It Fails**: Too broad, no clear scope, impossible to know when research is complete

**Fix**: "What ethical frameworks are applied to AI bias mitigation in hiring algorithms, and what evidence exists for their effectiveness?"

**Lesson**: Research questions should be specific, answerable, and bounded.

---

### Scope Creep

**Problem**: Starting with "AI in healthcare diagnostics" and expanding to "AI in all healthcare domains"

**Why It Fails**: Methodology becomes inadequate, timeline explodes, quality suffers

**Fix**: Define scope explicitly in definition.md with clear "what's excluded" section. Revisit scope only through deliberate refinement using `/research.refine`.

**Lesson**: Expanding scope requires methodology revision, not just "adding more sources."

---

### Cherry-Picking Sources

**Problem**: Only citing sources that support preferred conclusion

**Why It Fails**: Biased research, low credibility, misleading findings

**Fix**: Methodology must include "contradictory evidence" requirement. Explicitly seek and acknowledge sources that challenge findings.

**Lesson**: Research credibility depends on fair representation of evidence, not selective citation.

---

### Ignoring Contradictory Evidence

**Problem**: Finding evidence that conflicts with hypothesis and dismissing it without analysis

**Why It Fails**: Confirmation bias, incomplete understanding, weak conclusions

**Fix**: When contradictory evidence emerges, document it explicitly in findings. Consider alternative interpretations. State confidence levels (high/medium/low) based on evidence strength.

**Lesson**: Strong research acknowledges complexity and uncertainty.

---

### Weak Methodology

**Problem**: "I'll Google it and see what I find" without defined search strategy

**Why It Fails**: Unsystematic collection, non-reproducible, missed important sources

**Fix**: Define search terms, databases, inclusion/exclusion criteria before collection. Use systematic approach (PRISMA for academic research, structured framework for market research).

**Lesson**: Methodology defines research quality. Weak methodology = weak findings.

---

### Missing Citations

**Problem**: Making claims without citing sources, or citing sources but not maintaining references.bib

**Why It Fails**: Can't verify claims, academic dishonesty, low credibility

**Fix**: Cite as you go using BibTeX. Every claim traces to specific source in references.bib. Use citation management tools from the start.

**Lesson**: Attribution is non-negotiable. If you can't cite it, you can't claim it.

---

### Overgeneralizing

**Problem**: "All research shows AI improves healthcare outcomes" based on 3 studies

**Why It Fails**: Ignores limitations, misrepresents evidence strength, misleading conclusions

**Fix**: State findings precisely: "Three recent studies (Smith 2023, Jones 2024, Lee 2024) in radiology settings showed improved diagnostic accuracy. Generalizability to other healthcare domains unknown."

**Lesson**: Precision in claims reflects research rigor.

## Comparison with Traditional Research

| Aspect | Traditional Research | Research-Driven Investigation (RDI) |
|--------|---------------------|-------------------------------------|
| **Initiation** | Vague topic → ad-hoc exploration | Clear research question → structured definition |
| **Methodology** | Often implicit or post-hoc | Explicit, documented before execution |
| **Source Collection** | Opportunistic, inconsistent criteria | Systematic, defined inclusion/exclusion criteria |
| **Quality Standards** | Variable, researcher-dependent | Consistent, principles-driven |
| **Citation Management** | Afterthought, cleanup task | Real-time, BibTeX from the start |
| **Bias Mitigation** | Implicit, reviewer-dependent | Explicit strategies, documented in methodology |
| **Literature Review** | One-time phase | Continuous, living document |
| **Reproducibility** | Often difficult, methodology unclear | Transparent, methodology documented |
| **Scope Management** | Informal, prone to creep | Explicit boundaries, deliberate revision |
| **Analysis Framework** | Applied during analysis (implicit) | Defined before analysis (explicit) |
| **Alternative Explanations** | Rarely considered systematically | Required in analysis quality standards |
| **Limitations** | Often minimized or buried | Acknowledged prominently |
| **Progress Tracking** | Informal, personal notes | Structured task breakdown, version controlled |
| **Quality Assurance** | End-of-process review | Continuous, principle-based checkpoints |
| **AI Assistance** | Ad-hoc, "Google-style" queries | Systematic, methodology-driven investigation |
| **Collaboration** | Difficult, methodology not shared formally | Easy, git-based, methodology documented |
| **Evolution** | Research locked once started | Methodology can evolve with explicit versioning |

**Key Philosophical Difference**:

Traditional research treats methodology as a means to an end—something to get through to produce findings.

RDI treats methodology as the source of truth—findings are only as credible as the methodology that produced them.

**Practical Impact**:

In traditional research, when you find a contradictory source, you might ignore it or downplay it.

In RDI, your methodology requires acknowledging it, assessing its credibility, and considering alternative interpretations.

In traditional research, changing scope midstream means informally expanding your search.

In RDI, scope changes trigger methodology revision, version increment, and impact assessment.

## The Transformation

This isn't about replacing researchers or automating insight generation. It's about amplifying human research capability by automating systematic investigation while enforcing rigorous quality standards. It's about creating a tight feedback loop where research questions, methodologies, and evidence evolve together, each iteration bringing deeper understanding and more credible findings.

Research needs better tools for maintaining alignment between questions and investigation. RDI provides the methodology for achieving this alignment through executable research definitions and explicit methodologies that generate systematic investigation rather than merely guiding it.

The future of research is not ad-hoc exploration—it's structured, transparent, reproducible investigation where quality standards are explicit, bias mitigation is systematic, and findings trace directly back to rigorous methodology.

RDI makes this future possible today.
